#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIå­—å¹•ä¼˜åŒ–å™¨ - GLM å…¨å±€ä¸Šä¸‹æ–‡ç‰ˆæœ¬ (Global Context Version)

ç‰¹ç‚¹ï¼š
1. å…¨å±€ä¸Šä¸‹æ–‡æ„ŸçŸ¥ï¼šä¸å†é€è¡Œç¿»è¯‘ï¼Œè€Œæ˜¯å°†å¤§æ®µå­—å¹•å‘é€ç»™AIã€‚
2. æ™ºèƒ½é‡ç»„ (Re-segmentation)ï¼šAIè´Ÿè´£å°†ç ´ç¢çš„å­—å¹•è¡Œåˆå¹¶ä¸ºé€šé¡ºçš„å¥å­ã€‚
3. ç»“æ„åŒ–JSONè¾“å‡ºï¼šç¡®ä¿æ—¶é—´æˆ³å’Œå†…å®¹çš„ç²¾ç¡®å¯¹åº”ã€‚
4. ä¸“ä¸šæœ¯è¯­ä¿ç•™ï¼šå¼ºåˆ¶ä¿ç•™æŠ€æœ¯æœ¯è¯­ã€‚
"""

import os
import re
import json
import math
from pathlib import Path
from typing import List, Dict, Optional, Any
from dataclasses import dataclass
from dotenv import load_dotenv

# å°è¯•å¯¼å…¥ï¼Œå¦‚æœæ²¡æœ‰å®‰è£…åˆ™ç¨åæŠ¥é”™
try:
    from zhipuai import ZhipuAI
except ImportError:
    ZhipuAI = None

from utils import setup_logger, format_timestamp, parse_timestamp as parse_timestamp_str
from subtitle import parse_srt, SubtitleEntry as BaseSubtitleEntry

logger = setup_logger("subtitle_optimizer_glm_global")

@dataclass
class OptimizedEntry:
    """ä¼˜åŒ–åçš„å­—å¹•æ¡ç›®"""
    start_time: float
    end_time: float
    original_text: str
    translated_text: str

class GLMGlobalOptimizer:
    """ä½¿ç”¨æ™ºè°±AI GLMçš„å…¨å±€ä¸Šä¸‹æ–‡ä¼˜åŒ–å™¨"""
    
    def __init__(self, api_key: Optional[str] = None):
        load_dotenv()
        
        if not api_key:
            api_key = os.getenv("GLM_API_KEY")
        
        if not api_key:
            raise ValueError("æœªæ‰¾åˆ° GLM_API_KEYï¼Œè¯·åœ¨ .env ä¸­é…ç½®æˆ–ç›´æ¥ä¼ å…¥")
        
        if ZhipuAI is None:
            raise ImportError("è¯·å…ˆå®‰è£… zhipuai åŒ…: pip install zhipuai")
            
        self.client = ZhipuAI(api_key=api_key)
        self.model = "glm-4-flash" # ä½¿ç”¨ Flash æ¨¡å‹ï¼Œé€Ÿåº¦å¿«ä¸”ä¾¿å®œï¼Œé€‚åˆé•¿æ–‡æœ¬
        logger.info("âœ… GLM AI å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")

    def _format_batch_for_prompt(self, entries: List[BaseSubtitleEntry]) -> str:
        """å°†å­—å¹•æ¡ç›®åˆ—è¡¨æ ¼å¼åŒ–ä¸º Prompt æ–‡æœ¬"""
        lines = []
        for e in entries:
            start_str = format_timestamp(e.start_time)
            end_str = format_timestamp(e.end_time)
            # æ ¼å¼: [ID] start -> end: text
            lines.append(f"[{e.index}] {start_str} --> {end_str}: {e.text}")
        return "\n".join(lines)

    def _call_glm_api(self, prompt: str) -> str:
        """è°ƒç”¨ GLM API è·å–å“åº”"""
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1, # ä½æ¸©åº¦ä»¥ä¿è¯æ ¼å¼ç¨³å®š
                top_p=0.7,
            )
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"GLM API è°ƒç”¨å¤±è´¥: {e}")
            raise

    def _parse_json_response(self, response_text: str) -> List[Dict[str, Any]]:
        """è§£æ API è¿”å›çš„ JSON å­—ç¬¦ä¸²"""
        # æ¸…ç† Markdown ä»£ç å—æ ‡è®° ```json ... ```
        clean_text = re.sub(r'```json\s*', '', response_text)
        clean_text = re.sub(r'```\s*$', '', clean_text)
        clean_text = clean_text.strip()
        
        try:
            data = json.loads(clean_text)
            if not isinstance(data, list):
                raise ValueError("API è¿”å›çš„ä¸æ˜¯ JSON åˆ—è¡¨")
            return data
        except json.JSONDecodeError as e:
            logger.error(f"JSON è§£æå¤±è´¥. åŸå§‹å“åº”:\n{response_text}")
            raise ValueError(f"æ— æ³•è§£æ JSON: {e}")

    def optimize_batch(self, entries: List[BaseSubtitleEntry], context_summary: str = "") -> List[OptimizedEntry]:
        """ä¼˜åŒ–ä¸€æ‰¹å­—å¹•"""
        if not entries:
            return []

        formatted_input = self._format_batch_for_prompt(entries)
        
        prompt = f"""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è§†é¢‘å­—å¹•ä¼˜åŒ–ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯é‡ç»„å’Œç¿»è¯‘å­—å¹•ã€‚

ã€ä»»åŠ¡è¯´æ˜ã€‘
1. **é‡ç»„å¥å­ (Re-segmentation)**ï¼š
   - åŸå§‹å­—å¹•é€šå¸¸å°†ä¸€å¥è¯åˆ‡æ–­ã€‚è¯·æ ¹æ®è‹±è¯­è¯­æ³•å’Œä¸Šä¸‹æ–‡ï¼Œå°†ç ´ç¢çš„ç‰‡æ®µåˆå¹¶æˆå®Œæ•´çš„å¥å­ã€‚
   - ä¸è¦é—æ¼ä»»ä½•ä¿¡æ¯ï¼Œä¹Ÿä¸è¦é‡å¤ã€‚
2. **ç¿»è¯‘**ï¼š
   - å°†é‡ç»„åçš„å¥å­ç¿»è¯‘æˆæµç•…ã€è‡ªç„¶çš„ç®€ä½“ä¸­æ–‡ã€‚
   - **ä¸¥ç¦ç¿»è¯‘æŠ€æœ¯æœ¯è¯­**ï¼šå¿…é¡»ä¿ç•™ "Claude Code", "Cursor", "MCP", "AI", "API", "Python", "Agent" ç­‰è‹±æ–‡åŸè¯ã€‚
3. **ä¿ç•™æ—¶é—´æˆ³**ï¼š
   - å¯¹äºæ¯ä¸ªæ–°åˆæˆçš„å¥å­ï¼Œè®¡ç®—å…¶å¼€å§‹å’Œç»“æŸæ—¶é—´ã€‚
   - `start` å¿…é¡»æ˜¯è¯¥å¥ç¬¬ä¸€æ®µåŸæ–‡çš„å¼€å§‹æ—¶é—´ã€‚
   - `end` å¿…é¡»æ˜¯è¯¥å¥æœ€åä¸€æ®µåŸæ–‡çš„ç»“æŸæ—¶é—´ã€‚
4. **è¾“å‡ºæ ¼å¼**ï¼š
   - åªè¿”å›ä¸€ä¸ªçº¯ JSON æ•°ç»„ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ€§æ–‡å­—ã€‚
   - æ ¼å¼ç¤ºä¾‹ï¼š
     [
       {{
         "start": "00:00:00,240",
         "end": "00:00:04,000",
         "text": "Claude Code in 2026 is not what it was when it launched.",
         "translation": "2026å¹´çš„Claude Codeå·²éå½“åˆå‘å¸ƒæ—¶çš„æ¨¡æ ·ã€‚"
       }}
     ]

ã€è§†é¢‘ä¸»é¢˜èƒŒæ™¯ã€‘ï¼š{context_summary}

ã€åŸå§‹å­—å¹•æ•°æ®ã€‘ï¼š
{formatted_input}

è¯·å¼€å§‹å¤„ç†ï¼Œè¿”å› JSON æ•°æ®ï¼š
"""
        logger.info(f"ğŸ“¤ å‘é€æ‰¹æ¬¡è¯·æ±‚ (åŒ…å« {len(entries)} æ¡åŸå§‹å­—å¹•)...")
        response_text = self._call_glm_api(prompt)
        
        # è§£æç»“æœ
        try:
            json_data = self._parse_json_response(response_text)
            optimized_results = []
            
            for item in json_data:
                # è½¬æ¢å›å¯¹è±¡
                opt_entry = OptimizedEntry(
                    start_time=parse_timestamp_str(item['start']),
                    end_time=parse_timestamp_str(item['end']),
                    original_text=item['text'],
                    translated_text=item['translation']
                )
                optimized_results.append(opt_entry)
            
            logger.info(f"âœ… æ‰¹æ¬¡å¤„ç†æˆåŠŸï¼Œç”Ÿæˆ {len(optimized_results)} æ¡ä¼˜åŒ–å­—å¹•")
            return optimized_results
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹æ¬¡å¤„ç†å‡ºé”™: {e}")
            # å‡ºé”™é™çº§ç­–ç•¥ï¼šè‡³å°‘è¿”å›åŸæ–‡ï¼Œæˆ–è€…è¿™é‡Œå¯ä»¥ç›´æ¥æŠ›å‡ºè®©ä¸Šå±‚é‡è¯•
            # ä¸ºç®€å•èµ·è§ï¼Œè¿™é‡Œè¿”å›ç©ºåˆ—è¡¨ï¼Œç”±ä¸Šå±‚å¤„ç†
            return []

    def optimize_full_file(self, input_path: str, output_path: str, context: str = "", batch_size: int = 50):
        """ä¸»å…¥å£ï¼šä¼˜åŒ–æ•´ä¸ªæ–‡ä»¶"""
        logger.info(f"ğŸš€ å¼€å§‹å…¨å±€ä¸Šä¸‹æ–‡ä¼˜åŒ–: {input_path}")
        
        # 1. è¯»å–åŸå§‹å­—å¹•
        raw_entries = parse_srt(Path(input_path))
        logger.info(f"ğŸ“– è¯»å–åˆ° {len(raw_entries)} æ¡åŸå§‹å­—å¹•")
        
        # 2. åˆ†æ‰¹å¤„ç†
        # è™½ç„¶æ˜¯"å…¨å±€"ï¼Œä½†å—é™äº Token çª—å£ï¼Œæˆ‘ä»¬æŒ‰å¤§å—åˆ‡åˆ†
        # 50æ¡å­—å¹•é€šå¸¸çº¦ 1-3 åˆ†é’Ÿï¼Œè¶³ä»¥ä¿æŒå±€éƒ¨ä¸Šä¸‹æ–‡è¿è´¯
        all_optimized = []
        
        total_batches = math.ceil(len(raw_entries) / batch_size)
        
        for i in range(0, len(raw_entries), batch_size):
            batch_entries = raw_entries[i : i + batch_size]
            batch_idx = (i // batch_size) + 1
            
            logger.info(f"ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_idx}/{total_batches} ({len(batch_entries)} æ¡)...")
            
            results = self.optimize_batch(batch_entries, context)
            
            if not results:
                logger.warning(f"âš ï¸ æ‰¹æ¬¡ {batch_idx} å¤„ç†å¤±è´¥æˆ–æ— ç»“æœï¼Œå°è¯•é™çº§å¤„ç†...")
                # é™çº§ï¼šç›´æ¥æŠŠåŸå§‹çš„å¡è¿›å»ï¼Œé¿å…æ•´æ®µä¸¢å¤±
                for e in batch_entries:
                    all_optimized.append(OptimizedEntry(
                        start_time=e.start_time,
                        end_time=e.end_time,
                        original_text=e.text,
                        translated_text="[AIä¼˜åŒ–å¤±è´¥ï¼Œæœªç¿»è¯‘]" # æ ‡è®°ä¸€ä¸‹
                    ))
            else:
                all_optimized.extend(results)
                
                # æ‰“å°é¢„è§ˆ
                if results:
                    first = results[0]
                    logger.info(f"   ğŸ” é¢„è§ˆ: [{format_timestamp(first.start_time)}] {first.original_text} -> {first.translated_text}")

        # 3. ä¿å­˜ç»“æœ
        self._save_srt(all_optimized, output_path)
        logger.info(f"ğŸ’¾ ä¼˜åŒ–å®Œæˆï¼Œå·²ä¿å­˜è‡³: {output_path}")
        return True

    def _save_srt(self, entries: List[OptimizedEntry], path: str):
        """ä¿å­˜ä¸ºåŒè¯­ SRT æ ¼å¼"""
        with open(path, 'w', encoding='utf-8') as f:
            for i, entry in enumerate(entries):
                f.write(f"{i+1}\n")
                start = format_timestamp(entry.start_time)
                end = format_timestamp(entry.end_time)
                f.write(f"{start} --> {end}\n")
                
                # åŒè¯­æ ¼å¼ï¼šè‹±æ–‡åœ¨ä¸Šï¼Œä¸­æ–‡åœ¨ä¸‹ï¼ˆç¬¦åˆ Premium æ ·å¼è¦æ±‚ï¼‰
                # Premium æ ·å¼ä¼šè‡ªåŠ¨æŠŠä¸­æ–‡æ”¾ç¬¬ä¸€è¡Œ(å¦‚æœé…ç½®äº† chi_first)ï¼Œæˆ–è€…æ‰‹åŠ¨åœ¨æ­¤å¤„æ§åˆ¶
                # æŒ‰ç…§ä¹‹å‰çš„è§‚å¯Ÿï¼ŒPremium æ ·å¼æ˜¯è¯»å– SRT çš„å‰ä¸¤è¡Œ
                # æ‰€ä»¥æˆ‘ä»¬è¿™é‡Œå†™å…¥ï¼š
                # Line 1: è‹±æ–‡
                # Line 2: ä¸­æ–‡
                # è¿™æ · subtitle_generator.py å¯ä»¥æ­£å¸¸è§£æ
                
                f.write(f"{entry.original_text}\n")
                f.write(f"{entry.translated_text}\n\n")

if __name__ == "__main__":
    import sys
    
    # ç®€å•çš„å‘½ä»¤è¡Œå…¥å£
    try:
        if len(sys.argv) < 3:
            print("ç”¨æ³•: python subtitle_optimizer_glm.py <input_srt> <output_srt> [context]")
            print("\nç¤ºä¾‹: python subtitle_optimizer_glm.py input.srt output.srt 'Pythonæ•™ç¨‹'")
            sys.exit(1)
            
        input_file = sys.argv[1]
        output_file = sys.argv[2]
        ctx = sys.argv[3] if len(sys.argv) > 3 else "é€šç”¨è§†é¢‘"
        
        optimizer = GLMGlobalOptimizer()
        optimizer.optimize_full_file(input_file, output_file, ctx)
        
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
